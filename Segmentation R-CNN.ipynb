{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d66111d-d2b3-4664-8bb9-af4a443486f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Импорт необходимых библиотек\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Установка путей к директориям и файлам JSON\n",
    "# Замените 'path_to_dataset' на фактический путь к вашему набору данных\n",
    "DATA_DIR = 'FashionPedia'\n",
    "TRAIN_IMAGES_DIR = os.path.join(DATA_DIR, 'train')\n",
    "TEST_IMAGES_DIR = os.path.join(DATA_DIR, 'test')\n",
    "\n",
    "ATTRIBUTES_TRAIN_JSON = os.path.join(DATA_DIR, 'attributes_train2020.json')\n",
    "ATTRIBUTES_VAL_JSON = os.path.join(DATA_DIR, 'attributes_val2020.json')\n",
    "INSTANCES_TRAIN_JSON = os.path.join(DATA_DIR, 'instances_attributes_train2020.json')\n",
    "INSTANCES_VAL_JSON = os.path.join(DATA_DIR, 'instances_attributes_val2020.json')\n",
    "INFO_TEST_JSON = os.path.join(DATA_DIR, 'info_test2020.json')\n",
    "\n",
    "# Функция для загрузки JSON файла\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Загрузка данных из JSON файлов\n",
    "print(\"Загрузка JSON файлов...\")\n",
    "attributes_train = load_json(ATTRIBUTES_TRAIN_JSON)\n",
    "attributes_val = load_json(ATTRIBUTES_VAL_JSON)\n",
    "instances_train = load_json(INSTANCES_TRAIN_JSON)\n",
    "instances_val = load_json(INSTANCES_VAL_JSON)\n",
    "print(\"Загрузка завершена.\\n\")\n",
    "\n",
    "# Преобразование данных изображений в DataFrame\n",
    "def images_to_df(images_data):\n",
    "    images = images_data['images']\n",
    "    df = pd.DataFrame(images)\n",
    "    return df\n",
    "\n",
    "train_images_df = images_to_df(attributes_train)\n",
    "val_images_df = images_to_df(attributes_val)\n",
    "\n",
    "# Загрузка тестовых изображений, если файл существует\n",
    "if os.path.exists(INFO_TEST_JSON):\n",
    "    test_images_df = images_to_df(load_json(INFO_TEST_JSON))\n",
    "else:\n",
    "    test_images_df = pd.DataFrame()\n",
    "\n",
    "print(f\"Количество обучающих изображений: {len(train_images_df)}\")\n",
    "print(f\"Количество валидационных изображений: {len(val_images_df)}\")\n",
    "print(f\"Количество тестовых изображений: {len(test_images_df)}\\n\")\n",
    "\n",
    "# Преобразование аннотаций в DataFrame\n",
    "def annotations_to_df(annotations_data):\n",
    "    annotations = annotations_data['annotations']\n",
    "    ann_df = pd.DataFrame(annotations)\n",
    "    return ann_df\n",
    "\n",
    "train_annotations_df = annotations_to_df(instances_train)\n",
    "val_annotations_df = annotations_to_df(instances_val)\n",
    "\n",
    "print(f\"Количество аннотаций в обучающем наборе: {len(train_annotations_df)}\")\n",
    "print(f\"Количество аннотаций в валидационном наборе: {len(val_annotations_df)}\\n\")\n",
    "\n",
    "# Объединение изображений с аннотациями\n",
    "def merge_images_annotations(images_df, annotations_df):\n",
    "    merged_df = images_df.merge(annotations_df, left_on='id', right_on='image_id', how='left')\n",
    "    return merged_df\n",
    "\n",
    "train_merged_df = merge_images_annotations(train_images_df, train_annotations_df)\n",
    "val_merged_df = merge_images_annotations(val_images_df, val_annotations_df)\n",
    "\n",
    "print(\"Объединение изображений с аннотациями завершено.\\n\")\n",
    "\n",
    "# Загрузка атрибутов\n",
    "attributes_info = pd.concat([\n",
    "    pd.DataFrame(attributes_train['attributes']),\n",
    "    pd.DataFrame(attributes_val['attributes'])\n",
    "]).drop_duplicates(subset=['id']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Общее количество уникальных атрибутов: {len(attributes_info)}\\n\")\n",
    "\n",
    "# Пример вывода данных\n",
    "print(\"Пример данных из обучающего набора:\")\n",
    "display(train_merged_df.head())\n",
    "\n",
    "# Функция для отображения изображения с аннотациями\n",
    "def display_image_with_annotations(df, index=0, dataset_type='train'):\n",
    "    try:\n",
    "        row = df.iloc[index]\n",
    "    except IndexError:\n",
    "        print(f\"Индекс {index} выходит за пределы DataFrame.\")\n",
    "        return\n",
    "    \n",
    "    if dataset_type == 'train':\n",
    "        images_dir = TRAIN_IMAGES_DIR\n",
    "    elif dataset_type == 'test':\n",
    "        images_dir = TEST_IMAGES_DIR\n",
    "    else:\n",
    "        images_dir = TRAIN_IMAGES_DIR  # По умолчанию\n",
    "    \n",
    "    image_path = os.path.join(images_dir, row['file_name'])\n",
    "    \n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Изображение {image_path} не найдено.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при открытии изображения {image_path}: {e}\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Используем 'id_x' как Image ID и 'id_y' как Annotation ID\n",
    "    plt.title(f\"Image ID: {row['id_x']} | Annotation ID: {row['id_y']}\")\n",
    "    \n",
    "    # Проверяем наличие сегментаций\n",
    "    if 'segmentation' in row and isinstance(row['segmentation'], list):\n",
    "        for seg in row['segmentation']:\n",
    "            if isinstance(seg, list) and len(seg) >= 6:\n",
    "                try:\n",
    "                    # Преобразуем плоский список в список кортежей (x, y)\n",
    "                    xy = list(zip(seg[::2], seg[1::2]))\n",
    "                    polygon = plt.Polygon(xy, edgecolor='r', fill=False, linewidth=2)\n",
    "                    plt.gca().add_patch(polygon)\n",
    "                except Exception as e:\n",
    "                    print(f\"Ошибка при обработке сегментации: {e}\")\n",
    "            else:\n",
    "                print(f\"Некорректный формат сегментации: {seg}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "print(\"Отображение примера изображения с аннотациями:\")\n",
    "if not train_merged_df.empty:\n",
    "    display_image_with_annotations(train_merged_df, index=0, dataset_type='train')\n",
    "else:\n",
    "    print(\"Обучающий DataFrame пуст.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc595df-8413-4489-9f2e-90dd26d9c567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт стандартных библиотек\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "# Импорт PyTorch и torchvision\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms as T\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "# Импорт дополнительных библиотек\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Для обработки RLE\n",
    "# from pycocotools import mask as maskUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce3537a-1362-408e-8165-85b30064c35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение именованной функции collate_fn для DataLoader\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9491146c-705e-407a-b1fe-ae9cb256693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кастомные трансформации для обработки как изображений, так и целей\n",
    "\n",
    "class ToTensorTransform:\n",
    "    def __call__(self, image, target):\n",
    "        image = T.ToTensor()(image)\n",
    "        return image, target\n",
    "\n",
    "class RandomHorizontalFlipTransform:\n",
    "    def __init__(self, flip_prob):\n",
    "        self.flip_prob = flip_prob\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if np.random.rand() < self.flip_prob:\n",
    "            image = T.functional.hflip(image)\n",
    "            # Отражаем координаты боксов и масок\n",
    "            width = image.shape[2]\n",
    "            boxes = target[\"boxes\"]\n",
    "            if boxes.numel() > 0:\n",
    "                boxes[:, [0, 2]] = width - boxes[:, [2, 0]]\n",
    "                target[\"boxes\"] = boxes\n",
    "\n",
    "            # Отражение масок\n",
    "            masks = target[\"masks\"]\n",
    "            if masks.numel() > 0:\n",
    "                masks = masks.flip(-1)\n",
    "                target[\"masks\"] = masks\n",
    "\n",
    "        return image, target\n",
    "\n",
    "class ComposeTransforms:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1be532-ba65-47d7-aefe-5be1bde25299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, images_dir, attributes_info, transforms=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): DataFrame с аннотациями и информацией об изображениях.\n",
    "            images_dir (str): Путь к директории с изображениями.\n",
    "            attributes_info (pd.DataFrame): DataFrame с информацией об атрибутах.\n",
    "            transforms (callable, optional): Трансформации, применяемые к изображениям и целевым данным.\n",
    "        \"\"\"\n",
    "        self.images_dir = images_dir\n",
    "        self.attributes_info = attributes_info\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # Фильтруем строки, у которых существует соответствующий файл изображения\n",
    "        initial_len = len(dataframe)\n",
    "        dataframe['full_path'] = dataframe['file_name'].apply(lambda x: os.path.join(images_dir, x))\n",
    "        dataframe = dataframe[dataframe['full_path'].apply(os.path.exists)].reset_index(drop=True)\n",
    "        filtered_len = len(dataframe)\n",
    "        if filtered_len < initial_len:\n",
    "            print(f\"Пропущено {initial_len - filtered_len} изображений, так как файлы не найдены.\")\n",
    "        \n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Получаем строку DataFrame\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        \n",
    "        # Загружаем изображение с уже проверенным путем\n",
    "        img_path = row['full_path']\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при открытии изображения {img_path}: {e}. Пропуск.\")\n",
    "            raise RuntimeError(f\"Не удалось загрузить изображение: {img_path}\")\n",
    "\n",
    "        # Инициализируем целевые данные\n",
    "        target = {}\n",
    "        image_id = row['id_x']\n",
    "        target[\"image_id\"] = torch.tensor([image_id])\n",
    "        \n",
    "        # Получаем аннотации для изображения\n",
    "        annotations = self.dataframe[self.dataframe['id_x'] == image_id]\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        masks = []\n",
    "        areas = []\n",
    "        iscrowd = []\n",
    "        \n",
    "        for _, ann in annotations.iterrows():\n",
    "            # Боксы: преобразуем из [x_min, y_min, width, height] в [x_min, y_min, x_max, y_max]\n",
    "            x_min, y_min, width, height = ann['bbox']\n",
    "            x_max = x_min + width\n",
    "            y_max = y_min + height\n",
    "\n",
    "            # Проверка и коррекция координат BX\n",
    "            if x_max <= x_min or y_max <= y_min:\n",
    "                print(f\"Некорректный BBox: [x_min={x_min}, y_min={y_min}, x_max={x_max}, y_max={y_max}]. Пропуск аннотации.\")\n",
    "                continue  # Пропускаем некорректные боксы\n",
    "\n",
    "            boxes.append([x_min, y_min, x_max, y_max])\n",
    "            \n",
    "            # Метки (category_id)\n",
    "            labels.append(ann['category_id'])\n",
    "            \n",
    "            # Сегментации (преобразуем в маски)\n",
    "            segmentation = ann['segmentation']\n",
    "            mask = self.polygons_to_mask(segmentation, row['height'], row['width'])\n",
    "            masks.append(mask)\n",
    "            \n",
    "            # Площадь\n",
    "            areas.append(ann['area'])\n",
    "            \n",
    "            # iscrowd\n",
    "            iscrowd.append(ann['iscrowd'])\n",
    "        \n",
    "        if len(boxes) == 0:\n",
    "            # Если аннотаций нет, создаём пустые тензоры\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "            masks = torch.zeros((0, row['height'], row['width']), dtype=torch.uint8)\n",
    "            areas = torch.zeros((0,), dtype=torch.float32)\n",
    "            iscrowd = torch.zeros((0,), dtype=torch.int64)\n",
    "        else:\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            masks = torch.stack(masks)\n",
    "            areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "            iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"area\"] = areas\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "        \n",
    "        return img, target\n",
    "\n",
    "    def polygons_to_mask(self, polygons, height, width):\n",
    "        mask = Image.new('L', (width, height), 0)\n",
    "        for idx, polygon in enumerate(polygons):\n",
    "            # Проверяем, что polygon является списком\n",
    "            if not isinstance(polygon, list):\n",
    "                # print(f\"Проблема: Сегментация не является списком: {polygon}\")\n",
    "                continue  # Пропускаем некорректную сегментацию\n",
    "\n",
    "            # Проверяем, что все элементы polygon являются числами\n",
    "            if not all(isinstance(coord, (int, float)) for coord in polygon):\n",
    "                print(f\"Проблема: Некорректные координаты в сегментации: {polygon}\")\n",
    "                continue  # Пропускаем сегментацию с некорректными координатами\n",
    "\n",
    "            # Убедимся, что количество координат чётное\n",
    "            if len(polygon) % 2 != 0:\n",
    "                print(f\"Проблема: Нечётное количество координат в сегментации: {polygon}\")\n",
    "                continue  # Пропускаем сегментацию с нечётным числом координат\n",
    "\n",
    "            # Преобразуем плоский список координат в список кортежей\n",
    "            xy = list(zip(polygon[::2], polygon[1::2]))\n",
    "\n",
    "            # Проверяем, что длина xy достаточна для формирования полигона\n",
    "            if len(xy) < 3:\n",
    "                print(f\"Проблема: Недостаточно точек для полигона: {xy}\")\n",
    "                continue  # Пропускаем полигоны с недостаточным количеством точек\n",
    "\n",
    "            try:\n",
    "                ImageDraw.Draw(mask).polygon(xy=xy, outline=1, fill=1)\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка при рисовании полигона {xy}: {e}\")\n",
    "    \n",
    "        return torch.as_tensor(np.array(mask), dtype=torch.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08bcfc1-f44d-45af-a482-20bef22ba3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(ToTensorTransform())\n",
    "    if train:\n",
    "        transforms.append(RandomHorizontalFlipTransform(flip_prob=0.5))\n",
    "    return ComposeTransforms(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc42b62-1b42-465f-b678-c54657ea82fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание обучающего и валидационного наборов\n",
    "dataset_train = CustomDataset(train_merged_df, TRAIN_IMAGES_DIR, attributes_info, transforms=get_transform(train=True))\n",
    "dataset_val = CustomDataset(val_merged_df, TEST_IMAGES_DIR, attributes_info, transforms=get_transform(train=False))\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "def get_subset(dataset, fraction=0.1):\n",
    "    subset_size = int(len(dataset) * fraction)\n",
    "    subset_indices = list(range(len(dataset)))\n",
    "    np.random.shuffle(subset_indices)\n",
    "    return Subset(dataset, subset_indices[:subset_size])\n",
    "\n",
    "subset_train = get_subset(dataset_train, fraction=0.2)  # Используем 20% от оригинала\n",
    "subset_val = get_subset(dataset_val, fraction=0.2)      # Используем 20% от оригинала\n",
    "\n",
    "# Создание подмножеств для тестового прогона (например, 1% от тренировочного и валидационного наборов)\n",
    "subset_train_test = get_subset(dataset_train, fraction=0.01)\n",
    "subset_val_test = get_subset(dataset_val, fraction=0.01)\n",
    "\n",
    "\n",
    "data_loader_train = DataLoader(\n",
    "    subset_train_test,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "data_loader_val = DataLoader(\n",
    "    subset_val_test,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "print(\"DataLoader-ы созданы успешно.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7b5eb2-27a0-458a-96e8-4ccae528467e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "# Функция для инициализации модели Mask R-CNN\n",
    "def get_instance_segmentation_model(num_classes):\n",
    "    # Загружаем предобученную модель Mask R-CNN\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "    # Получаем количество входных признаков для классификатора\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # Замещаем классификатор на новый с num_classes выходами\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    # Получаем количество входных признаков для масочного предиктора\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    \n",
    "    # Замещаем масочный предиктор новым\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Определяем количество классов (например, количество категориний + фон)\n",
    "unique_categories = train_annotations_df['category_id'].unique()\n",
    "print(unique_categories)\n",
    "print(f\"Максимальный category_id: {unique_categories.max()}\")\n",
    "\n",
    "num_classes = unique_categories.max() + 1\n",
    "\n",
    "# Инициализируем модель\n",
    "model = get_instance_segmentation_model(num_classes)\n",
    "\n",
    "# Перемещаем модель на GPU, если доступен\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Модель инициализирована и перемещена на устройство: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8bdf07-dc9b-4ad1-a239-e8cb773c1b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настройка оптимизатора\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(params, lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Использование ReduceLROnPlateau\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n",
    "\n",
    "print(\"Оптимизатор и планировщик обучения настроены.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e014d01b-834c-4c37-a6b0-f2100f4a537d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from collections import defaultdict, deque\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=100):\n",
    "    model.train()\n",
    "    metric_logger = defaultdict(deque)\n",
    "    progress_bar = tqdm(enumerate(data_loader), total=len(data_loader), desc=f\"Epoch {epoch}\")\n",
    "\n",
    "    for i, (images, targets) in progress_bar:\n",
    "        if images is None or targets is None:\n",
    "            print(f\"Пропуск батча {i} из-за некорректных данных.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            for k, v in loss_dict.items():\n",
    "                metric_logger[k].append(v.item())\n",
    "\n",
    "            if i % print_freq == 0:\n",
    "                metrics_str = ' | '.join([f\"{k}: {np.mean(v):.4f}\" for k, v in metric_logger.items()])\n",
    "                progress_bar.set_postfix_str(f\"{metrics_str}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка на батче {i}: {e}\")\n",
    "            raise e  # Прерываем обучение для отладки\n",
    "\n",
    "\n",
    "    return metric_logger\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, targets) in enumerate(data_loader):\n",
    "            if images is None or targets is None:\n",
    "                print(f\"Пропуск батча {i} из-за некорректных данных.\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                images = list(img.to(device) for img in images)\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "                loss_dict = model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "                total_loss += losses.item()\n",
    "                num_batches += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка на батче {i}: {e}\")\n",
    "                raise e  # Прерываем оценку для отладки\n",
    "\n",
    "    average_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0597fadf-7110-40e3-8bd2-3ecbe25b62c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Запуск эпохи {epoch}\")\n",
    "    train_metrics = train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=100)\n",
    "\n",
    "    # Вычисляем валидационные метрики\n",
    "    val_loss = evaluate(model, data_loader_val, device)\n",
    "    print(f\"Валидационная потеря: {val_loss:.4f}\")\n",
    "\n",
    "    # Передаём валидационную потерю в планировщик\n",
    "    lr_scheduler.step(val_loss)\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    print(f\"Epoch #{epoch} завершено за {epoch_time:.2f} секунд.\")\n",
    "\n",
    "    # Выводим потери на обучении\n",
    "    train_losses = {k: f\"{np.mean(v):.4f}\" for k, v in train_metrics.items()}\n",
    "    print(f\"Потери на обучении: {train_losses}\")\n",
    "\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5595051b-05ee-4243-a3c0-367779af3a35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Сохранение state_dict модели\n",
    "torch.save(model.state_dict(), \"mask_rcnn_state_dict.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da2d0a3-0d6e-4911-9228-b7f6a70d2992",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import coremltools as ct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ce6847-d6e4-4049-8bd1-18d6bd8b9cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка модели\n",
    "model = get_instance_segmentation_model(num_classes)\n",
    "model.load_state_dict(torch.load(\"mask_rcnn_state_dict.pth\", map_location='cpu'))\n",
    "model.eval()\n",
    "\n",
    "# Перемещение модели на CPU, если она была обучена на GPU\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3353e8-cb03-48c7-853a-1a9fb53acf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание фиктивного ввода, соответствующего размеру входных данных модели\n",
    "dummy_input = torch.randn(1, 3, 800, 800)  # Обновите размер в соответствии с вашими данными\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56929bae-2fef-4c6c-acd9-70c90deb610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Трассировка модели\n",
    "scripted_model = torch.jit.trace(model, dummy_input)\n",
    "\n",
    "# Сохранение TorchScript модели (опционально)\n",
    "scripted_model.save(\"mask_rcnn_scripted.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5b6ff1-fbe9-4eb7-b164-5c00cb99c296",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Конвертация TorchScript модели в Core ML\n",
    "mlmodel = ct.convert(\n",
    "    scripted_model,\n",
    "    inputs=[ct.ImageType(name=\"input_image\", shape=dummy_input.shape)]\n",
    ")\n",
    "\n",
    "# Сохранение Core ML модели\n",
    "mlmodel.save(\"mask_rcnn.mlmodel\")\n",
    "\n",
    "print(\"Модель успешно сконвертирована в формат Core ML и сохранена как mask_rcnn.mlmodel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173d4118-961c-4ea6-ad04-c6003107da71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(f\"Версия PyTorch: {torch.__version__}\")\n",
    "print(f\"Версия torchvision: {torchvision.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361875e6-9219-4fd1-b5cd-1ca76566c208",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
